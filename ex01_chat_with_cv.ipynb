{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e92d0c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c70b092",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GG_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17aa4608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2c5036",
   "metadata": {},
   "source": [
    "1. Load document\n",
    "2. Convert to text\n",
    "3. Chunking\n",
    "4. Embed\n",
    "5. Store \n",
    "6. Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8def4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "loader = PyPDFLoader('NguyenThienNhan_CV.pdf')\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 100\n",
    ")\n",
    "chunks = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7feb294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thnhan301/anaconda3/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "embeddings_model = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03ddc857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!docker run     --name pgvector-container     -e POSTGRES_USER=langchain     -e POSTGRES_PASSWORD=langchain     -e POSTGRES_DB=langchain     -p 6024:5432     -d pgvector/pgvector:pg16\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "!docker run \\\n",
    "    --name pgvector-container \\\n",
    "    -e POSTGRES_USER=langchain \\\n",
    "    -e POSTGRES_PASSWORD=langchain \\\n",
    "    -e POSTGRES_DB=langchain \\\n",
    "    -p 6024:5432 \\\n",
    "    -d pgvector/pgvector:pg16\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5677ccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = \"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e800cb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_postgres.vectorstores import PGVector\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be1caaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = PGVector.from_documents(chunks, embeddings_model, connection=conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7719bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='1d051d5f-560d-41ab-9005-d78f0b23f578', metadata={'page': 0, 'title': \"Nhan's CV\", 'author': 'Thien Nhan', 'source': 'NguyenThienNhan_CV.pdf', 'creator': 'LaTeX with RenderCV', 'moddate': '2025-06-25T12:48:35+00:00', 'subject': '', 'trapped': '/False', 'keywords': '', 'producer': 'pdfTeX-1.40.26', 'page_label': '', 'total_pages': 1, 'creationdate': '2025-06-25T12:48:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0'}, page_content='Nguyen Thien Nhan\\nAI Engineer\\nHo Chi Minh | nhanthien.tnn@gmail.com | 30/11/2003 | 0328980403 | github.com/nhanth301| Linkedin\\nSummary\\nAnalytical AI Engineer with a passion for rigorous research and creative problem-solving. Skilled in transforming complex\\nrequirements into innovative, data-driven solutions. Seeking to apply these abilities to solve challenging real-world problems.\\nSkills\\nLanguages & Frameworks:C++, Python, SQL, PyTorch, Scikit-learn, NumPy , Pandas, OpenCV\\nMachine Learning:Ensemble Methods (XGBoost, Random Forest), Clustering, PCA, Regression\\nNLP, LLMs & Generative AI:NMT, NER, QA; LLM Fine-tuning, RAG; GANs\\nComputer Vision:Image (Classification, Segmentation), Object Detection, Super Resolution\\nEnglish: TOEIC 780/990\\nEducation\\nHo Chi Minh City University of Technology and Education Sep 2021 – 2025\\n• Major: Information Technology | GPA: 3.31/4.0 | Graduation Thesis Score: 9.2/10.0\\nAIO 2023 - AI Viet Nam Sep 2023 – May 2024'),\n",
       " Document(id='d67057bc-9130-465a-820b-b49b1575e027', metadata={'page': 0, 'title': \"Nhan's CV\", 'author': 'Thien Nhan', 'source': 'NguyenThienNhan_CV.pdf', 'creator': 'LaTeX with RenderCV', 'moddate': '2025-06-25T12:48:35+00:00', 'subject': '', 'trapped': '/False', 'keywords': '', 'producer': 'pdfTeX-1.40.26', 'page_label': '', 'total_pages': 1, 'creationdate': '2025-06-25T12:48:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0'}, page_content='Nguyen Thien Nhan\\nAI Engineer\\nHo Chi Minh | nhanthien.tnn@gmail.com | 30/11/2003 | 0328980403 | github.com/nhanth301| Linkedin\\nSummary\\nAnalytical AI Engineer with a passion for rigorous research and creative problem-solving. Skilled in transforming complex\\nrequirements into innovative, data-driven solutions. Seeking to apply these abilities to solve challenging real-world problems.\\nSkills\\nLanguages & Frameworks:C++, Python, SQL, PyTorch, Scikit-learn, NumPy , Pandas, OpenCV\\nMachine Learning:Ensemble Methods (XGBoost, Random Forest), Clustering, PCA, Regression\\nNLP, LLMs & Generative AI:NMT, NER, QA; LLM Fine-tuning, RAG; GANs\\nComputer Vision:Image (Classification, Segmentation), Object Detection, Super Resolution\\nEnglish: TOEIC 780/990\\nEducation\\nHo Chi Minh City University of Technology and Education Sep 2021 – 2025\\n• Major: Information Technology | GPA: 3.31/4.0 | Graduation Thesis Score: 9.2/10.0\\nAIO 2023 - AI Viet Nam Sep 2023 – May 2024'),\n",
       " Document(id='3e49e2fb-bc60-47ef-abf4-5bd022ab08a3', metadata={'page': 0, 'title': \"Nhan's CV\", 'author': 'Thien Nhan', 'source': 'NguyenThienNhan_CV.pdf', 'creator': 'LaTeX with RenderCV', 'moddate': '2025-06-25T12:48:35+00:00', 'subject': '', 'trapped': '/False', 'keywords': '', 'producer': 'pdfTeX-1.40.26', 'page_label': '', 'total_pages': 1, 'creationdate': '2025-06-25T12:48:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0'}, page_content='AIO 2023 - AI Viet Nam Sep 2023 – May 2024\\n• The course established a robust foundational knowledge in AI, from classical ML algorithms to modern DL architectures.\\nWork Experience\\nViettel High Technology Industries Corporation\\nInternship\\nJan 2025 – Jun 2025\\n• Built a system to detect DDoS attacksby using an Autoencoder model to find unusual patterns in network activity .\\n• Built a super-resolution modelto improve license plate recognition. Focused on solving the data problem by creating a\\nmethod to generate paired training data from real-world, unpaired images.\\n• Researched the new Mamba architectureand presented its advantages to the team, explaining how its efficiency could\\nmake it a future replacement for Transformers.\\nProjects\\nReal-World License Plate Enhancement and Recognition[GitHub]\\nMar 2025 - Jun 2025\\n• Engineered a novel data synthesis pipelineusing a hybrid of CycleGAN, estimated real-world kernels, and'),\n",
       " Document(id='e1f1e96c-eca0-4d74-8a5a-c99b60498c80', metadata={'page': 0, 'title': \"Nhan's CV\", 'author': 'Thien Nhan', 'source': 'NguyenThienNhan_CV.pdf', 'creator': 'LaTeX with RenderCV', 'moddate': '2025-06-25T12:48:35+00:00', 'subject': '', 'trapped': '/False', 'keywords': '', 'producer': 'pdfTeX-1.40.26', 'page_label': '', 'total_pages': 1, 'creationdate': '2025-06-25T12:48:35+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.26 (TeX Live 2024) kpathsea version 6.4.0'}, page_content='AIO 2023 - AI Viet Nam Sep 2023 – May 2024\\n• The course established a robust foundational knowledge in AI, from classical ML algorithms to modern DL architectures.\\nWork Experience\\nViettel High Technology Industries Corporation\\nInternship\\nJan 2025 – Jun 2025\\n• Built a system to detect DDoS attacksby using an Autoencoder model to find unusual patterns in network activity .\\n• Built a super-resolution modelto improve license plate recognition. Focused on solving the data problem by creating a\\nmethod to generate paired training data from real-world, unpaired images.\\n• Researched the new Mamba architectureand presented its advantages to the team, explaining how its efficiency could\\nmake it a future replacement for Transformers.\\nProjects\\nReal-World License Plate Enhancement and Recognition[GitHub]\\nMar 2025 - Jun 2025\\n• Engineered a novel data synthesis pipelineusing a hybrid of CycleGAN, estimated real-world kernels, and')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.similarity_search(\"What is education of this applicant\",k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d219f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import chain\n",
    "template = PromptTemplate.from_template(\"\"\"\n",
    "    Answer the question based on the context provided\n",
    "    Context: {context}\n",
    "    Question: {question}\n",
    "    Answer:\n",
    "\"\"\")\n",
    "@chain \n",
    "def chat_with_docs(question):\n",
    "    topk_docs = db.similarity_search(question, k=2)\n",
    "    context = \"\"\n",
    "    for doc in topk_docs:\n",
    "        context += '\\n' + doc.page_content \n",
    "    print(context)\n",
    "    print(\"=============\")\n",
    "    prompt = template.invoke({'context' : context, 'question' : question})\n",
    "    return llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fb500a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "content preservation across multiple, sequential style applications.\n",
      "• Outperformed pure Mamba (MambaST) and Transformer (StyTr²)architectures in content preservation, achieving a\n",
      "superior Content Feature Structural Distance (CFSD) of 0.2551.\n",
      "• Dominated serial style transfer benchmarksby reducing perceptual error (LPIPS) by 61.4% and improving structural\n",
      "similarity (SSIM) by 48.9% compared to previous works.\n",
      "Smart Public Library System[GitHub]\n",
      "Oct 2024 - Dec 2024\n",
      "• Developed an end-to-end smart library systemfeaturing a real-time recommendation engine using Reinforcement\n",
      "Learning (REINFORCE) and Contrastive Learning to personalize user experience.\n",
      "• Engineered a real-time data pipelinewith Apache Kafka and Quix Stream to process live user interactions, feeding\n",
      "dynamic data into the recommendation model.\n",
      "• Implemented a smart chatbot using Retrieval-Augmented Generation (RAG)to provide users with intuitive,\n",
      "context-aware book suggestions based on natural language queries.\n",
      "content preservation across multiple, sequential style applications.\n",
      "• Outperformed pure Mamba (MambaST) and Transformer (StyTr²)architectures in content preservation, achieving a\n",
      "superior Content Feature Structural Distance (CFSD) of 0.2551.\n",
      "• Dominated serial style transfer benchmarksby reducing perceptual error (LPIPS) by 61.4% and improving structural\n",
      "similarity (SSIM) by 48.9% compared to previous works.\n",
      "Smart Public Library System[GitHub]\n",
      "Oct 2024 - Dec 2024\n",
      "• Developed an end-to-end smart library systemfeaturing a real-time recommendation engine using Reinforcement\n",
      "Learning (REINFORCE) and Contrastive Learning to personalize user experience.\n",
      "• Engineered a real-time data pipelinewith Apache Kafka and Quix Stream to process live user interactions, feeding\n",
      "dynamic data into the recommendation model.\n",
      "• Implemented a smart chatbot using Retrieval-Augmented Generation (RAG)to provide users with intuitive,\n",
      "context-aware book suggestions based on natural language queries.\n",
      "=============\n",
      "The applicant developed an end-to-end smart library system (Oct 2024 - Dec 2024) featuring a real-time recommendation engine using Reinforcement Learning and Contrastive Learning. They also engineered a real-time data pipeline with Apache Kafka and Quix Stream and implemented a smart chatbot using Retrieval-Augmented Generation (RAG) for context-aware book suggestions.\n"
     ]
    }
   ],
   "source": [
    "print(chat_with_docs.invoke(\"Describe briefly about Smart Public Library System project of the applicant\").content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff810fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "class Answer(BaseModel):\n",
    "    \"\"\"An answer as brief summary along with rating\"\"\"\n",
    "    summary : str\n",
    "    \"\"\"Summary of a project\"\"\"\n",
    "    technologies : str\n",
    "    \"\"\"the technologies have been used in the project\"\"\"\n",
    "structured_llm = llm.with_structured_output(Answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c99735fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@chain \n",
    "def chat_with_docs(question):\n",
    "    topk_docs = db.similarity_search(question, k=2)\n",
    "    context = \"\"\n",
    "    for doc in topk_docs:\n",
    "        context += '\\n' + doc.page_content \n",
    "    prompt = template.invoke({'context' : context, 'question' : question})\n",
    "    return structured_llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca5e0a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The applicant developed an end-to-end smart library system featuring a real-time recommendation engine using Reinforcement Learning (REINFORCE) and Contrastive Learning to personalize user experience. They also engineered a real-time data pipeline with Apache Kafka and Quix Stream to process live user interactions, feeding dynamic data into the recommendation model. Additionally, they implemented a smart chatbot using Retrieval-Augmented Generation (RAG) to provide users with intuitive, context-aware book suggestions based on natural language queries.\n",
      "Reinforcement Learning (REINFORCE), Contrastive Learning, Apache Kafka, Quix Stream, Retrieval-Augmented Generation (RAG\n"
     ]
    }
   ],
   "source": [
    "result = chat_with_docs.invoke(\"Describe briefly about Smart Public Library System project of the applicant\")\n",
    "print(result.summary)\n",
    "print(result.technologies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff13274f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.indexes import SQLRecordManager, index\n",
    "from langchain.docstore.document import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d48d8abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = \"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\"\n",
    "collection_name = \"my_docs\"\n",
    "embeddings_model = HuggingFaceEmbeddings()\n",
    "namespace = \"my_docs_namespace\"\n",
    "\t\n",
    "vectorstore = PGVector(\n",
    "    embeddings=embeddings_model,\n",
    "    collection_name=collection_name,\n",
    "    connection=connection,\n",
    "    use_jsonb=True,\n",
    ")\n",
    "\t\n",
    "record_manager = SQLRecordManager(\n",
    "    namespace,\n",
    "    db_url=\"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\",\n",
    ")\n",
    "\n",
    "record_manager.create_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8c54b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    Document(page_content='there are cats in the pond', metadata={\n",
    "        \"id\": 1, \"source\": \"cats.txt\"}),\n",
    "    Document(page_content='ducks are also found in the pond', metadata={\n",
    "        \"id\": 2, \"source\": \"ducks.txt\"}),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c21e5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index attempt 1: {'num_added': 2, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}\n"
     ]
    }
   ],
   "source": [
    "# Index the documents\n",
    "index_1 = index(\n",
    "    docs,\n",
    "    record_manager,\n",
    "    vectorstore,\n",
    "    cleanup=\"incremental\",  # prevent duplicate documents\n",
    "    source_id_key=\"source\",  # use the source field as the source_id\n",
    ")\n",
    "\t\n",
    "print(\"Index attempt 1:\", index_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6032e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index attempt 2: {'num_added': 0, 'num_updated': 0, 'num_skipped': 2, 'num_deleted': 0}\n"
     ]
    }
   ],
   "source": [
    "# second time you attempt to index, it will not add the documents again\n",
    "index_2 = index(\n",
    "    docs,\n",
    "    record_manager,\n",
    "    vectorstore,\n",
    "    cleanup=\"incremental\",\n",
    "    source_id_key=\"source\",\n",
    ")\n",
    "\t\n",
    "print(\"Index attempt 2:\", index_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e5d5a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index attempt 3: {'num_added': 1, 'num_updated': 0, 'num_skipped': 1, 'num_deleted': 1}\n"
     ]
    }
   ],
   "source": [
    "docs[0].page_content = \"I just modified this document!\"\n",
    "\t\n",
    "index_3 = index(\n",
    "    docs,\n",
    "    record_manager,\n",
    "    vectorstore,\n",
    "    cleanup=\"incremental\",\n",
    "    source_id_key=\"source\",\n",
    ")\n",
    "\t\n",
    "print(\"Index attempt 3:\", index_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa446187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_added': 1, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 2}\n"
     ]
    }
   ],
   "source": [
    "docs = [Document(page_content='there are dogs in the pond', metadata={\"id\": 1, \"source\": \"dogs.txt\"})]\n",
    "\n",
    "index_4 = index(\n",
    "    docs,\n",
    "    record_manager,\n",
    "    vectorstore,\n",
    "    cleanup='full',\n",
    "    source_id_key='source'\n",
    ")\n",
    "print(index_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cdfee2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
