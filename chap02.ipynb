{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ad3e17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv('GG_API_KEY')\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6ffc236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, Language\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.documents import Document\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9cc7cd",
   "metadata": {},
   "source": [
    "# MultiVectorRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8117fcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = \"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\"\n",
    "collection_name = \"summaries\"\n",
    "embeddings_model = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b70b3767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of loaded docs:  15157\n"
     ]
    }
   ],
   "source": [
    "loader = TextLoader(\"./readme.md\", encoding=\"utf-8\")\n",
    "docs = loader.load()\n",
    "\t\n",
    "print(\"length of loaded docs: \", len(docs[0].page_content))\n",
    "# Split the document\n",
    "splitter = RecursiveCharacterTextSplitter.from_language(language=Language.MARKDOWN, chunk_size=2000, chunk_overlap=200)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\t\n",
    "# The rest of your code remains the same, starting from:\n",
    "prompt_text = \"Summarize the following document:\\n\\n{doc}\"\n",
    "\t\n",
    "prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "summarize_chain = {\n",
    "    \"doc\": lambda x: x.page_content} | prompt | llm | StrOutputParser()\n",
    "\t\n",
    "# batch the chain across the chunks\n",
    "summaries = summarize_chain.batch(chunks, {\"max_concurrency\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efffc52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = PGVector(\n",
    "    embeddings=embeddings_model,\n",
    "    collection_name=collection_name,\n",
    "    connection=connection,\n",
    "    use_jsonb=True,\n",
    ")\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "id_key = \"doc_id\"\n",
    "\t\n",
    "# indexing the summaries in our vector store, whilst retaining the original \n",
    "# documents in our document store:\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "\t\n",
    "# Changed from summaries to chunks since we need same length as docs\n",
    "doc_ids = [str(uuid.uuid4()) for _ in chunks]\n",
    "\n",
    "# Each summary is linked to the original document by the doc_id\n",
    "summary_docs = [Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(summaries)\n",
    "]\n",
    "\n",
    "# Add the document summaries to the vector store for similarity search\n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "\t\n",
    "# Store the original documents in the document store, linked to their summaries \n",
    "# via doc_ids\n",
    "# This allows us to first search summaries efficiently, then fetch the full \n",
    "# docs when needed\n",
    "retriever.docstore.mset(list(zip(doc_ids, chunks)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32b63a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document highlights that while the project successfully demonstrated the SR module's effectiveness, the overall pipeline's speed (FPS) was low due to unoptimized detection and recognition models (YOLOv5). Future work should focus on optimizing the entire pipeline for speed by: using lighter YOLOv5 versions, applying model optimization techniques like quantization and pruning, converting models to TensorRT, refactoring the codebase for better modularity and reduced bottlenecks, and implementing a multi-threaded/asynchronous pipeline to parallelize tasks and maximize GPU utilization.\n",
      "\n",
      "\n",
      "This document acknowledges the contributions of two key resources to the project. First, it credits GitHub user **chequanghuy** for their work on license plate detection and OCR models based on the YOLOv5 framework, specifically referencing their repository \"[chequanghuy/Character-Time-series-Matching](https://github.com/chequanghuy/Character-Time-series-Matching)\". Second, it thanks AI assistants like Google's Gemini and OpenAI's ChatGPT for their help in debugging, code refactoring, and documentation.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# vector store retrieves the summaries\n",
    "sub_docs = retriever.vectorstore.similarity_search(\n",
    "    \"name of the project\", k=2)\n",
    "for doc in sub_docs:\n",
    "    print(doc.page_content)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5f5ae58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.  **Optimizing the Full Pipeline for Speed (FPS)**\n",
      "    This project's primary focus was to prove the efficacy of the SR module. Consequently, the detection and recognition models (YOLOv5) were not optimized for inference speed, resulting in a low end-to-end FPS. Future work could focus on performance optimization, including:\n",
      "    * **Utilizing lighter model backbones:** Employing more lightweight YOLOv5 versions (e.g., YOLOv5n, YOLOv5s) for the detection and OCR tasks.\n",
      "    * **Applying model optimization techniques:** Using methods such as quantization and pruning to reduce the computational complexity of the models.\n",
      "    * **Converting models to TensorRT:** Migrating the optimized models to NVIDIA's TensorRT engine to maximize inference throughput on target GPU hardware.\n",
      "    * **Codebase Refactoring:** Refactoring the core pipeline for improved modularity, reducing I/O bottlenecks, and enhancing overall code quality for better maintainability and extensibility.\n",
      "    * **Implementing a Multi-threaded/Asynchronous Pipeline:** Parallelizing I/O-bound tasks (like video frame reading) with GPU-bound tasks (model inference). This de-couples the components and can significantly improve overall throughput by ensuring the GPU is never idle.\n",
      "===========\n",
      "\n",
      "### Quanlitative Results\n",
      "Below are examples where the Super-Resolution model significantly improved OCR results compared to the original low-resolution image.\n",
      "\n",
      "<img src=\"imgs/sr_convincing_improvements.png\" alt=\"demo\">\n",
      "===========\n",
      "\n",
      "## Introduction\n",
      "\n",
      "In real-world applications, license plate images captured by surveillance cameras are often low-resolution, blurry, and noisy due to weather conditions, lighting, motion, or distance. A major challenge in developing robust super-resolution models is the lack of paired dataâ€”corresponding high-resolution (HR) and low-resolution (LR) images from the same scene.\n",
      "\n",
      "Traditional methods create synthetic LR images by applying simple mathematical degradations (like Gaussian blur or down-sampling) to HR images. However, this approach produces artificial LR images that fail to capture the diversity and complexity of real-world quality degradation factors.\n",
      "\n",
      "This project introduces a **Hybrid Degradation Pipeline** to generate more realistic HR-LR data pairs, which are then used to train an effective Super-Resolution model for the specific task of improving OCR accuracy.\n",
      "\n",
      "## Methodology\n",
      "\n",
      "The project workflow consists of two main stages: data generation, SR model training.\n",
      "===========\n",
      "\n",
      "## Acknowledgements\n",
      "\n",
      "The plate detection and Optical Character Recognition (OCR) models used in this project are based on the YOLOv5 framework and were adapted from or utilized methodologies from the work of GitHub user **chequanghuy**.\n",
      "\n",
      "* **Source Repository:** [chequanghuy/Character-Time-series-Matching](https://github.com/chequanghuy/Character-Time-series-Matching)\n",
      "\n",
      "We would like to thank the author for their valuable work and for sharing a helpful project for the license plate recognition task in the Vietnamese context.\n",
      "\n",
      "Additionally, a special thanks to the AI assistants, including Google's Gemini and OpenAI's ChatGPT, which were instrumental throughout the development process. Their assistance in debugging complex errors, refactoring code, and writing documentation was invaluable.\n",
      "===========\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(\"what is the methodology of the project\")\n",
    "for doc in retrieved_docs:\n",
    "    print(doc.page_content)\n",
    "    print('===========\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
