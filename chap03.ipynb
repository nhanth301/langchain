{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e64e0159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv('GG_API_KEY')\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8caea49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "import uuid\n",
    "loader = PyPDFLoader('NguyenThienNhan_CV.pdf')\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 100\n",
    ")\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "embeddings_model = HuggingFaceEmbeddings()\n",
    "conn = \"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\"\n",
    "db = PGVector.from_documents(chunks, embeddings_model, connection=conn)\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673cd0b6",
   "metadata": {},
   "source": [
    "# 1. Query Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874d4cab",
   "metadata": {},
   "source": [
    "### Rewrite-Retrieve-Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa6fa212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"project name\" features OR \"project name\" highlights OR \"project name\" key functionalities'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the question based only on \n",
    "    the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\")\n",
    "\n",
    "rewrite_prompt = ChatPromptTemplate.from_template(\"\"\"Provide a better search query for deep dive into a project decription, anwser just only one query.\n",
    "                                                  Question: {x} Answer:\"\"\")\n",
    "def parse_rewrite_output(message):\n",
    "    return message.content.strip(\"**\").strip()\n",
    "rewriter = rewrite_prompt | llm | parse_rewrite_output\n",
    "\n",
    "# rewriter.invoke({'x' : \"Today I woke up and brushed my teeth, then I sat down to read the news. But then I forgot the food on the cooker. Who are some key figures in the ancient greek history of philosophy?\"})\n",
    "rewriter.invoke('Some interesting features of this project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42ec6ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, the strongest technology used in the library project is MLOps tools including Docker, Docker Compose, and Apache Airflow.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import chain\n",
    "\n",
    "@chain\n",
    "def qa_rrr(input):\n",
    "    new_query = rewriter.invoke({'x' : input})\n",
    "    docs = retriever.invoke(new_query)\n",
    "    formatted = prompt.invoke({'context' : docs, 'question' : input})\n",
    "    return llm.invoke(formatted)\n",
    "\n",
    "print(qa_rrr.invoke(\"Which is the strongest technology has used in library project\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e7c62f",
   "metadata": {},
   "source": [
    "### Multi-Query Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "877b88fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "perspectives_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    You are an AI language \n",
    "    model assistant. Your task is to generate five different versions of the \n",
    "    given user question to retrieve relevant documents from a vector database. \n",
    "    By generating multiple perspectives on the user question, your goal is to \n",
    "    help the user overcome some of the limitations of the distance-based \n",
    "    similarity search. Provide these alternative questions separated by \n",
    "    newlines. Original question: {question}\n",
    "\"\"\")\n",
    "\n",
    "def parse_queries_output(message):\n",
    "    return message.content.split(\"\\n\")\n",
    "\n",
    "query_gen = perspectives_prompt | llm | parse_queries_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "294af259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_union(document_lists):\n",
    "    dedupled_docs = {\n",
    "        doc.page_content : doc for sublist in document_lists for doc in sublist\n",
    "    }\n",
    "    return list(dedupled_docs.values())\n",
    "retrieval_chain = query_gen | retriever.batch | get_unique_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdf0453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@chain\n",
    "def multi_query_qa(input):\n",
    "    docs = retrieval_chain.invoke(input)\n",
    "    formatted = prompt.invoke({'context' : docs, 'question' : input})\n",
    "    return llm.invoke(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "692a282f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, the strongest technologies used in the library project are:\n",
      "\n",
      "*   Reinforcement Learning (REINFORCE)\n",
      "*   Contrastive Learning\n",
      "*   Apache Kafka\n",
      "*   Quix Stream\n",
      "*   Retrieval-Augmented Generation (RAG)\n",
      "*   MLOps tools including Docker, Docker Compose, and Apache Airflow\n"
     ]
    }
   ],
   "source": [
    "print(multi_query_qa.invoke(\"Which is the strongest technology has used in library project\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22d46d8",
   "metadata": {},
   "source": [
    "### RAG-Fusion (Multi-query Retrieval + Reciprocal Rank Fusion RRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eded9915",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_rag_fusion = ChatPromptTemplate.from_template(\"\"\"You are a helpful \n",
    "    assistant that generates multiple search queries based on a single input \n",
    "    query. \\n\n",
    "    Generate multiple search queries related to: {question} \\n\n",
    "    Output (4 queries):\"\"\")\n",
    "\n",
    "def parse_queries_output(message):\n",
    "    return message.content.split('\\n')\n",
    "\n",
    "query_gen = prompt_rag_fusion | llm | parse_queries_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ebf3dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank_fusion(results, k=60):\n",
    "    fused_scores = {}\n",
    "    documents = {}\n",
    "\n",
    "    for docs in results:\n",
    "        for rank, doc in enumerate(docs):\n",
    "            doc_str = doc.page_content\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "                documents[doc_str] = doc\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "    reranked_doc_strs = sorted(fused_scores, key=lambda d: fused_scores[d], reverse=True)\n",
    "    return [documents[doc_str] for doc_str in reranked_doc_strs]\n",
    "\n",
    "retrieval_chain = query_gen | retriever.batch | reciprocal_rank_fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9482c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@chain \n",
    "def multi_query_qa(input):\n",
    "    docs = retrieval_chain.invoke(input)\n",
    "    formatted = prompt.invoke({'context' : docs, 'question' : input})\n",
    "    return llm.invoke(formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39e5ce59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided documents, the strongest technologies used in the library project appear to be:\n",
      "\n",
      "*   **Reinforcement Learning (REINFORCE) and Contrastive Learning:** Used for a real-time recommendation engine to personalize user experience.\n",
      "*   **Apache Kafka and Quix Stream:** Used for a real-time data pipeline to process live user interactions.\n",
      "*   **Retrieval-Augmented Generation (RAG):** Used to implement a smart chatbot for context-aware book suggestions.\n",
      "*   **MLOps tools (Docker, Docker Compose, and Apache Airflow):** Used to orchestrate the entire system.\n"
     ]
    }
   ],
   "source": [
    "print(multi_query_qa.invoke(\"Which is the strongest technology has used in library project\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62380223",
   "metadata": {},
   "source": [
    "### HyDE (Hypothetical Document Embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0f82280",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "prompt_hyde = ChatPromptTemplate.from_template(\"\"\"Please write a passage to \n",
    "   answer the question.\\n Question: {question} \\n Passage:\"\"\")\n",
    "\n",
    "generate_doc = (\n",
    "    prompt_hyde | llm | StrOutputParser() \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c3e2fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain = generate_doc | retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4849edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@chain\n",
    "def qa(input):\n",
    "  docs = retrieval_chain.invoke(input)\n",
    "  formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
    "  answer = llm.invoke(formatted)\n",
    "  return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bad10154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, the strongest technology used in the Smart Public Library System project is Retrieval-Augmented Generation (RAG).\n"
     ]
    }
   ],
   "source": [
    "print(qa.invoke(\"Which is the strongest technology has used in library project\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f4961f",
   "metadata": {},
   "source": [
    "# 2. Query Routing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21bcd07",
   "metadata": {},
   "source": [
    "### Logical Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "daafb53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "    datasource: Literal[\"python_docs\", \"js_docs\"] = Field(\n",
    "        ...,\n",
    "        description=\"\"\"Given a user question, choose which datasource would be \n",
    "            most relevant for answering their question\"\"\",\n",
    "    )\n",
    "\n",
    "structured_llm = llm.with_structured_output(RouteQuery)\n",
    "system = \"\"\"\"You are an expert at routing a user question to the appropriate data \n",
    "    source Based on the programming language the question is referring to, route it to the \n",
    "    relevant data source.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\",\"{question}\")\n",
    "    ]\n",
    ")\n",
    "router = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d307fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python_docs'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"\"\"Why doesn't the following code work:\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"])\n",
    "prompt.invoke(\"french\")\n",
    "\"\"\"\n",
    "\n",
    "result = router.invoke({\"question\": question})\n",
    "\n",
    "result.datasource"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c8ba12",
   "metadata": {},
   "source": [
    "### Semantic Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfa075e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utils.math import cosine_similarity\n",
    "# Two prompts\n",
    "physics_template = \"\"\"You are a very smart physics professor. You are great at \n",
    "    answering questions about physics in a concise and easy-to-understand manner. \n",
    "    When you don't know the answer to a question, you admit that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. You are great at answering \n",
    "    math questions. You are so good because you are able to break down hard \n",
    "    problems into their component parts, answer the component parts, and then \n",
    "    put them together to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25b431e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_templates = [physics_template, math_template]\n",
    "prompt_embeddings = embeddings_model.embed_documents(prompt_templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b3ff4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ah, a black hole! Excellent question.\n",
      "\n",
      "In essence, a black hole is a region in spacetime where gravity is so strong that nothing, not even light, can escape. Think of it like a cosmic vacuum cleaner with incredibly powerful suction.\n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "*   **Formation:** Black holes typically form when massive stars die. When a star runs out of fuel, it collapses under its own gravity. If the star is massive enough, this collapse continues until it forms a singularity – a point of infinite density.\n",
      "\n",
      "*   **Event Horizon:** Surrounding the singularity is the event horizon. This is the \"point of no return.\" Anything that crosses the event horizon is doomed to be pulled into the singularity.\n",
      "\n",
      "*   **Gravity:** The immense gravity of a black hole warps spacetime around it. This is what causes the extreme effects we observe, like the bending of light.\n",
      "\n",
      "*   **Types:** Black holes come in different sizes:\n",
      "\n",
      "    *   **Stellar Black Holes:** Formed from the collapse of individual stars.\n",
      "    *   **Supermassive Black Holes:** Found at the centers of most galaxies, with masses millions or even billions of times that of the Sun.\n",
      "    *   **Intermediate-Mass Black Holes:** A less common type, with masses between stellar and supermassive black holes.\n",
      "    *   **Primordial Black Holes:** Hypothetical black holes that may have formed in the early universe.\n",
      "\n",
      "In short, a black hole is an incredibly dense object with gravity so strong that nothing can escape its pull. They are fascinating objects that continue to be a major area of research in astrophysics.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "# Route question to prompt\n",
    "@chain\n",
    "def prompt_router(query):\n",
    "    # Embed question\n",
    "    query_embedding = embeddings_model.embed_query(query)\n",
    "    # Compute similarity\n",
    "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
    "    # Pick the prompt most similar to the input question\n",
    "    most_similar = prompt_templates[similarity.argmax()]\n",
    "    return PromptTemplate.from_template(most_similar)\n",
    "\n",
    "semantic_router = (\n",
    "    prompt_router\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(semantic_router.invoke(\"What's a black hole\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16651fd7",
   "metadata": {},
   "source": [
    "# 3. Query Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b699004c",
   "metadata": {},
   "source": [
    "#### Text-to-Metadata Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd7a1c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b14778ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "fields = [\n",
    "    AttributeInfo(\n",
    "        name=\"genre\",\n",
    "        description=\"The genre of the movie\",\n",
    "        type=\"string or list[string]\",\n",
    "    ),  AttributeInfo(\n",
    "        name=\"year\",\n",
    "        description=\"The year the movie was released\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"director\",\n",
    "        description=\"The name of the movie director\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"rating\", description=\"A 1-10 rating for the movie\", type=\"float\"\n",
    "    ),\n",
    "]\n",
    "description = \"Brief summary of a movie\"\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm, db, description, fields,\n",
    ")\n",
    "\n",
    "print(retriever.invoke(\n",
    "    \"What's a highly rated (above 8.5) science fiction film?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
